{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Video Time: 1\n",
      "Video Time: 2\n",
      "Video Time: 3\n",
      "Video Time: 4\n",
      "Video Time: 5\n",
      "Video Time: 6\n",
      "Video Time: 7\n",
      "Video Time: 8\n",
      "Video Time: 9\n",
      "Video Time: 10\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-abaad15271fd>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    249\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    250\u001b[0m \u001b[1;31m# run code\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 251\u001b[1;33m \u001b[0mdetection_video_file\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvideo_path\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweightsPath\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconfigPath\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcoco_names\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0.5\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0.5\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-1-abaad15271fd>\u001b[0m in \u001b[0;36mdetection_video_file\u001b[1;34m(video_path, yolo_weights, yolo_cfg, coco_names, confidence_threshold, nms_threshold)\u001b[0m\n\u001b[0;32m    232\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    233\u001b[0m     \u001b[0mh\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mw\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mframe\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 234\u001b[1;33m     \u001b[0mboxes\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconfidences\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mclass_ids\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mperform_detection\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnet\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mframe\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutput_layers\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mw\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mh\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconfidence_threshold\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    235\u001b[0m     \u001b[0mindexes\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdraw_boxes\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mboxes\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconfidences\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mclass_ids\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mclasses\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mframe\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcolors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconfidence_threshold\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnms_threshold\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    236\u001b[0m     \u001b[0mhigh_risk\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlow_risk\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msafe\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0midf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhigh_risk_cord\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlow_risk_cord\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcalculate_dist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mboxes\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindexes\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-1-abaad15271fd>\u001b[0m in \u001b[0;36mperform_detection\u001b[1;34m(net, img, output_layers, w, h, confidence_threshold)\u001b[0m\n\u001b[0;32m     61\u001b[0m     \u001b[0mblob\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mblobFromImage\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m \u001b[1;33m/\u001b[0m \u001b[1;36m255.\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;36m416\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m416\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mswapRB\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcrop\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m     \u001b[0mnet\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msetInput\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mblob\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 63\u001b[1;33m     \u001b[0mlayer_outputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnet\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput_layers\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     64\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     65\u001b[0m     \u001b[0mboxes\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "  \n",
    "import cv2\n",
    "import time\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "weightsPath = \"./yolov3.weights\"\n",
    "configPath = \"./yolov3.cfg\"\n",
    "coco_names = \"./coco.names\"\n",
    "LABELS = open(coco_names).read().strip().split(\"\\n\")\n",
    "video_path = \"./videos/video.mp4\"\n",
    "\n",
    "\n",
    "\n",
    "def load_input_image(image_path):\n",
    "    test_img = cv2.imread(image_path)\n",
    "    h, w, _ = test_img.shape\n",
    "    return test_img, h, w\n",
    "\n",
    "\n",
    "\n",
    "def yolov3(yolo_weights, yolo_cfg, coco_names):\n",
    "    net = cv2.dnn.readNet(yolo_weights, yolo_cfg)\n",
    "    classes = open(coco_names).read().strip().split(\"\\n\")\n",
    "    layer_names = net.getLayerNames()\n",
    "    output_layers = [layer_names[i[0] - 1] for i in net.getUnconnectedOutLayers()]\n",
    "\n",
    "    return net, classes, output_layers\n",
    "\n",
    "\n",
    "\n",
    "def get_calibrate(p1,p2):\n",
    "  dist0 = p1[0] - p2[0]\n",
    "  sum1 = p1[1] + p2[1]\n",
    "  dist1 = p1[1] - p2[1]\n",
    "  calibrate = (dist0 ** 2 + (550 / (sum1 / 2)) * (dist1 ** 2)) ** 0.5\n",
    "  return calibrate\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "  0 => Safe\n",
    "  1 => Low Risk\n",
    "  2 => High Risk\n",
    "\"\"\"\n",
    "def distance(p1,p2) -> int:\n",
    "  cd = get_calibrate(p1,p2)\n",
    "  calibrate = (p1[1] + p2[1])/2\n",
    "  if 0 < cd < 0.15 * calibrate:\n",
    "    return 2\n",
    "  elif 0 < cd < 0.20 * calibrate:\n",
    "    return 1\n",
    "  else:\n",
    "    return 0\n",
    "\n",
    "\n",
    "progress = 0\n",
    "count = 0\n",
    "\n",
    "\n",
    "def perform_detection(net, img, output_layers, w, h, confidence_threshold):\n",
    "    global progress, count\n",
    "    blob = cv2.dnn.blobFromImage(img, 1 / 255., (416, 416), swapRB=True, crop=False)\n",
    "    net.setInput(blob)\n",
    "    layer_outputs = net.forward(output_layers)\n",
    "\n",
    "    boxes = []\n",
    "    confidences = []\n",
    "    class_ids = []\n",
    "    person_cords = []\n",
    "\n",
    "    for output in layer_outputs:\n",
    "        for detection in output:\n",
    "            scores = detection[5:]\n",
    "            class_id = np.argmax(scores)\n",
    "            confidence = scores[class_id]\n",
    "\n",
    "            if LABELS[class_id] == 'person':\n",
    "              if confidence > confidence_threshold:\n",
    "                  center_x, center_y, width, height = list(map(int, \n",
    "                  \tdetection[0:4] * [w, h, w, h]))\n",
    "                  # print(\"centerX: \", center_x)\n",
    "                  person_cords.append((center_x,center_y))\n",
    "                \n",
    "                  top_left_x = int(center_x - (width / 2))\n",
    "                  top_left_y = int(center_y - (height / 2))\n",
    "\n",
    "                  boxes.append([top_left_x, top_left_y, width, height])\n",
    "                  confidences.append(float(confidence))\n",
    "                  class_ids.append(class_id)\n",
    "    \n",
    "    if progress == 24:\n",
    "      count += 1\n",
    "      print(\"Video Time: {}\".format(count))\n",
    "      progress = 0\n",
    "    else:\n",
    "      progress += 1\n",
    "\n",
    "    \n",
    "    # print(\"person_cords: \", person_cords)\n",
    "    return boxes, confidences, class_ids\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def draw_boxes(boxes, confidences, class_ids, classes, img, \n",
    "\tcolors, confidence_threshold, NMS_threshold):\n",
    "\n",
    "    indexes = cv2.dnn.NMSBoxes(boxes, confidences, confidence_threshold, NMS_threshold)\n",
    "    FONT = cv2.FONT_HERSHEY_SIMPLEX\n",
    "\n",
    "    if len(indexes) > 0:\n",
    "        for i in indexes.flatten():\n",
    "            x, y, w, h = boxes[i]\n",
    "            cv2.rectangle(img, (x, y), (x + w, y + h), (169,121,25), 2)\n",
    "            # cv2.circle(img, (int(x+(w/2)), int(y+(h/2))), 2, (0,255,0), 2 )\n",
    "            # text = \"{}: {:.4f}\".format(classes[class_ids[i]], confidences[i])\n",
    "            # cv2.putText(img, text, (x, y - 5), FONT, 0.5, color, 2)\n",
    "\n",
    "    return indexes\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def detection_image_file(image_path, yolo_weights, yolo_cfg, coco_names, confidence_threshold, nms_threshold):\n",
    "\n",
    "    img, h, w = load_input_image(image_path)\n",
    "    net, classes, output_layers = yolov3(yolo_weights, yolo_cfg, coco_names)\n",
    "    colors = np.random.uniform(0, 255, size=(len(classes), 3))\n",
    "    boxes, confidences, class_ids = perform_detection(net, img, output_layers, w, h, confidence_threshold)\n",
    "    draw_boxes(boxes, confidences, class_ids, classes, img, colors, confidence_threshold, nms_threshold)\n",
    "\n",
    "    cv2.waitKey(0)\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def calculate_dist(boxes, indexes):\n",
    "\n",
    "  if len(indexes) > 0:\n",
    "    idf = indexes.flatten()\n",
    "    centers = list()\n",
    "    status = list()\n",
    "\n",
    "    safe = list()\n",
    "    low_risk = list()\n",
    "    high_risk = list()\n",
    "\n",
    "    for i in idf:\n",
    "      (x,y) = (boxes[i][0],boxes[i][1]) # top-left position\n",
    "      (w,h) = (boxes[i][2],boxes[i][3])\n",
    "      centers.append([int(x+(w/2)), int(y+(h/2))])\n",
    "      status.append(0)\n",
    "    \n",
    "    cr = range(len(centers))\n",
    "    for c1 in cr:\n",
    "      for c2 in cr:\n",
    "        dst = distance(centers[c1], centers[c2])\n",
    "        if dst == 2:\n",
    "          high_risk.append([centers[c1], centers[c2]])\n",
    "          status[c1] = 2\n",
    "          status[c2] = 2\n",
    "        elif dst == 1:\n",
    "          low_risk.append([centers[c1], centers[c2]])\n",
    "          if status[c1] != 2:\n",
    "            status[c1] = 1\n",
    "          if status[c2] != 2:\n",
    "            status[c2] = 1\n",
    "\n",
    "    person_count = len(centers)\n",
    "    high_risk_count = status.count(2)\n",
    "    low_risk_count = status.count(1)\n",
    "    safe_count = status.count(0)\n",
    "    return high_risk_count, low_risk_count, safe_count, idf, high_risk, low_risk\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def draw_distance(frame, idf, boxes, WIDTH, HEIGHT, high_risk, low_risk, \n",
    "\tsafe, high_risk_cord, low_risk_cord):\n",
    "\n",
    "  for i in idf:\n",
    "    sub_img = frame[600:HEIGHT, 0:200]\n",
    "    black_rect = np.ones(sub_img.shape, dtype=np.uint8) * 0\n",
    "    res = cv2.addWeighted(sub_img, 0.70, black_rect, 0.30, 1.0)\n",
    "    frame[600:HEIGHT, 0:200] = res\n",
    "    \"\"\"\n",
    "    sub_img = frame[10:520, 230:710]\n",
    "    frame[10:520, 230:710] = res\n",
    "    \"\"\"\n",
    "    cv2.putText(frame, \"TOTAL    : {}\".format(high_risk+low_risk+safe),(25, HEIGHT - 90),\n",
    "                        cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 255, 255), 2)\n",
    "    cv2.putText(frame, \"HIGH RISK: {}\".format(high_risk),(25, HEIGHT - 65),\n",
    "                        cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 0, 150), 2)\n",
    "    cv2.putText(frame, \"LOW RISK : {}\".format(low_risk), (25, HEIGHT - 40),\n",
    "                        cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 120, 255), 2)\n",
    "    cv2.putText(frame, \"SAFE     : {}\".format(safe), (25, HEIGHT - 15),\n",
    "                        cv2.FONT_HERSHEY_SIMPLEX, 0.6,  (255, 255, 255), 2)\n",
    "    \n",
    "  for l in high_risk_cord:\n",
    "    cv2.line(frame, tuple(l[0]), tuple(l[1]), (10, 44, 236), 2)\n",
    "  for h in low_risk_cord:\n",
    "    cv2.line(frame, tuple(h[0]), tuple(h[1]), (10, 236, 236), 2)\n",
    "  \n",
    "  return frame\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def detection_video_file(video_path, yolo_weights, yolo_cfg, coco_names, \n",
    "\tconfidence_threshold, nms_threshold):\n",
    "\n",
    "  net, classes, output_layers = yolov3(yolo_weights, yolo_cfg, coco_names)\n",
    "  colors = np.random.uniform(0, 255, size=(len(classes), 3))\n",
    "  video = cv2.VideoCapture(video_path)\n",
    "  writer = None\n",
    "  (HEIGHT,WIDTH) = (None, None)\n",
    "  MAX_WIDTH = 0\n",
    "\n",
    "  while True:\n",
    "    (ret, frame) = video.read()\n",
    "\n",
    "    if not ret:\n",
    "      break\n",
    "      \n",
    "    if WIDTH is None or HEIGHT is None:\n",
    "      (HEIGHT,WIDTH) = frame.shape[:2]\n",
    "      MAX_WIDTH = WIDTH\n",
    "    \n",
    "    frame = frame[0:HEIGHT, 200:MAX_WIDTH]\n",
    "    (HEIGHT,WIDTH) = frame.shape[:2]\n",
    "    \n",
    "    h, w = frame.shape[:2]\n",
    "    boxes, confidences, class_ids = perform_detection(net, frame, output_layers, w, h, confidence_threshold)\n",
    "    indexes = draw_boxes(boxes, confidences, class_ids, classes, frame, colors, confidence_threshold, nms_threshold)\n",
    "    high_risk, low_risk, safe, idf, high_risk_cord, low_risk_cord = calculate_dist(boxes, indexes)\n",
    "    draw_distance(frame, idf, boxes, WIDTH, HEIGHT, high_risk, low_risk, safe, high_risk_cord, low_risk_cord)\n",
    "\n",
    "    if writer is None:\n",
    "        fourcc = cv2.VideoWriter_fourcc(*\"XVID\")\n",
    "        writer = cv2.VideoWriter(\"output.avi\", fourcc, 30.0, (frame.shape[1], frame.shape[0]))\n",
    "    writer.write(frame)\n",
    "    \n",
    "\n",
    "  writer.release()\n",
    "  video.release()\n",
    "\n",
    "\n",
    "\n",
    "# run code\n",
    "detection_video_file(video_path, weightsPath, configPath, coco_names, 0.5, 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
